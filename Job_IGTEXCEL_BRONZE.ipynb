{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dcb804d-e9f0-4c89-93dd-38a08ff2340d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../initializeJobVariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bfae1d2-c831-4a40-bd2d-5ec2931d69a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "def clean_column_name(name: str) -> str:\n",
    "    cleaned = re.sub(r\"[\\s\\-\\/]+\", \"_\", name.strip())\n",
    "    cleaned = re.sub(r\"_+\", \"_\", cleaned).strip(\"_\")\n",
    "    return cleaned\n",
    "\n",
    "def dedupe_names(names):\n",
    "    seen = {}\n",
    "    deduped = []\n",
    "    for name in names:\n",
    "        if name not in seen:\n",
    "            seen[name] = 0\n",
    "            deduped.append(name)\n",
    "        else:\n",
    "            seen[name] += 1\n",
    "            deduped.append(f\"{name}_{seen[name]}\")\n",
    "    return deduped\n",
    "\n",
    "\n",
    "def normalize_columns(df):\n",
    "    cleaned = [clean_column_name(sanitize_identifier(c)) for c in df.columns]\n",
    "    cleaned = dedupe_names(cleaned)\n",
    "    for original, new_name in zip(df.columns, cleaned):\n",
    "        if original != new_name:\n",
    "            df = df.withColumnRenamed(original, new_name)\n",
    "    return df\n",
    "\n",
    "def derive_table_name(workbook_file_name: str) -> str:\n",
    "    base = os.path.splitext(os.path.basename(workbook_file_name))[0]\n",
    "    return clean_column_name(base)\n",
    "\n",
    "\n",
    "def copy_to_dbfs_tmp(source_path: str) -> str:\n",
    "    tmp_name = f\"{uuid.uuid4().hex}.xlsx\"\n",
    "    dbfs_tmp = f\"dbfs:/tmp/{tmp_name}\"\n",
    "    dbutils.fs.cp(source_path, dbfs_tmp, True)\n",
    "    return dbfs_tmp\n",
    "\n",
    "def dbfs_to_local(dbfs_path: str) -> str:\n",
    "    return \"/dbfs/\" + dbfs_path.replace(\"dbfs:/\", \"\")\n",
    "    \n",
    "def get_local_workbook(source_path: str):\n",
    "    dbfs_tmp = copy_to_dbfs_tmp(source_path)\n",
    "    return dbfs_tmp, dbfs_to_local(dbfs_tmp)\n",
    "\n",
    "def get_sheet_names(local_path: str):\n",
    "    xls = pd.ExcelFile(local_path)\n",
    "    return xls.sheet_names\n",
    "\n",
    "def sanitize_identifier(name: str) -> str:\n",
    "    # Remove accents\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Replace invalid characters\n",
    "    name = re.sub(r'[^a-zA-Z0-9_]', '_', name)\n",
    "    \n",
    "    # Ensure doesn't start with digit\n",
    "    if name and name[0].isdigit():\n",
    "        name = \"_\" + name\n",
    "        \n",
    "    return name\n",
    "    \n",
    "def clean_col(c):\n",
    "   return re.sub(r'[^a-zA-Z0-9_]', '_', c)\n",
    "\n",
    "\n",
    "def detect_data_sheet(local_path: str, explicit_sheet: str) -> str:\n",
    "    if explicit_sheet:\n",
    "        return explicit_sheet\n",
    " \n",
    "    try:\n",
    "        sheet_names = get_sheet_names(local_path)\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\n",
    "            \"Unable to auto-detect sheet names. Set data_sheet_name.\"\n",
    "        ) from exc\n",
    "\n",
    " \n",
    "    best_sheet = None\n",
    "    best_count = -1\n",
    "    failures = []\n",
    " \n",
    "    for sheet in sheet_names:\n",
    "        try:\n",
    "            df = pd.read_excel(local_path, sheet_name=sheet, header=0)\n",
    "            if df.empty:\n",
    "                continue\n",
    " \n",
    "            non_null_count = df.notna().any(axis=1).sum()\n",
    " \n",
    "            if non_null_count > best_count:\n",
    "                best_count = non_null_count\n",
    "                best_sheet = sheet\n",
    "        except Exception as exc:\n",
    "            failures.append((sheet, str(exc)))\n",
    "            continue\n",
    " \n",
    "    if best_sheet is None:\n",
    "        raise RuntimeError(\n",
    "            \"No suitable data sheet found. Set data_sheet_name explicitly. \"\n",
    "            f\"Sheet read failures: {failures}\"\n",
    "        )\n",
    " \n",
    "    return best_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fad836b1-a578-4b24-831b-89756cf13dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "from pyspark.sql.functions import current_date\n",
    "from datetime import datetime,timezone\n",
    "from pyspark.sql import functions as F\n",
    "import uuid\n",
    "from pyspark.sql.functions import col\n",
    "dict_job_data = getRunDetails()\n",
    "pipeline_name = dict_job_data[\"jobName\"]\n",
    "job_id = dict_job_data[\"jobId\"]\n",
    "job_run_id = dict_job_data[\"jobRunId\"]\n",
    "start_time = datetime.now(timezone.utc).isoformat()\n",
    "environment = \"development\"\n",
    "try:\n",
    "    sql = f\"SELECT pipeline_id FROM {environment}_011_bronze_core.db_admin.pipeline WHERE pipeline_name = '{pipeline_name}'\"\n",
    "    df = spark.sql(sql)\n",
    "    new_id_row = df.collect()\n",
    "    pipeline_id = new_id_row[0][\"pipeline_id\"] if new_id_row else None\n",
    "    sql = f\"SELECT variable_value FROM {environment}_011_bronze_core.db_admin.pipeline_parameter WHERE pipeline_id = '{pipeline_id}' and variable_name = 'workbook_path'\"\n",
    "    df = spark.sql(sql)\n",
    "    new_id_row = df.collect()\n",
    "    master_path = new_id_row[0][\"variable_value\"] if new_id_row else None\n",
    "    sql = f\"SELECT variable_value FROM {environment}_011_bronze_core.db_admin.pipeline_parameter WHERE pipeline_id = '{pipeline_id}' and variable_name = 'db_catalog'\"\n",
    "    df = spark.sql(sql)\n",
    "    new_id_row = df.collect()\n",
    "    db_schema_name = new_id_row[0][\"variable_value\"] if new_id_row else None\n",
    "    sql = f\"SELECT etl_id FROM {environment}_011_bronze_core.db_admin.pipeline_history WHERE job_run_id = '{job_run_id}'\"\n",
    "    df = spark.sql(sql)\n",
    "    new_id_row = df.collect()\n",
    "    new_id = new_id_row[0][\"etl_id\"] if new_id_row else None\n",
    "    pdf = master_path\n",
    "    file_name = pdf.split(\"/\")[-1]\n",
    "    sql = f\"SELECT file_id, file_name FROM {environment}_011_bronze_core.db_admin.pipeline_file WHERE file_name = '{file_name}'\"\n",
    "    df_p_file = spark.sql(sql)\n",
    "    table_name = db_schema_name + \".\" + clean_column_name(file_name.replace(\".xlsx\",\"\").replace(\".xls\",\"\"))\n",
    "    file_row = df_p_file.collect()\n",
    "    file_exists = file_row and file_row[0][\"file_name\"] is not None\n",
    "    \n",
    "    if not file_exists:\n",
    "        sql = f\"\"\"\n",
    "        INSERT INTO {environment}_011_bronze_core.db_admin.pipeline_file\n",
    "        (etl_id, file_name, file_path, file_date, file_import, file_status, created_date)\n",
    "        VALUES (\n",
    "        try_cast({new_id} as bigint),\n",
    "        '{file_name}',\n",
    "        '{pdf}',\n",
    "        current_date(),\n",
    "        TRUE,\n",
    "        'Active',\n",
    "        current_timestamp()\n",
    "        )\n",
    "        \"\"\"\n",
    "        spark.sql(sql)\n",
    "    else:\n",
    "        sql = f\"\"\"\n",
    "        UPDATE {environment}_011_bronze_core.db_admin.pipeline_file\n",
    "        SET file_status = 'Deleted'\n",
    "        WHERE file_name = '{file_name}'\n",
    "        \"\"\"\n",
    "        spark.sql(sql)\n",
    "        sql = f\"\"\"\n",
    "        INSERT INTO {environment}_011_bronze_core.db_admin.pipeline_file\n",
    "        (etl_id, file_name, file_path, file_date, file_import, file_status, created_date)\n",
    "        VALUES (\n",
    "        try_cast({new_id} as bigint),\n",
    "        '{file_name}',\n",
    "        '{master_path}',\n",
    "        current_date(),\n",
    "        TRUE,\n",
    "        'Active',\n",
    "        current_timestamp()\n",
    "        )\n",
    "        \"\"\"\n",
    "        spark.sql(sql)\n",
    "    sql = f\"SELECT file_id, file_name FROM {environment}_011_bronze_core.db_admin.pipeline_file WHERE file_name = '{file_name}'\"\n",
    "    df_p_file = spark.sql(sql)\n",
    "    file_row = df_p_file.collect()\n",
    "    file_id = file_row[0][\"file_id\"] \n",
    "    print(master_path)\n",
    "    dbfs_tmp, local_path = get_local_workbook(master_path)\n",
    "    sql = f\"SELECT variable_value FROM {environment}_011_bronze_core.db_admin.pipeline_parameter WHERE pipeline_id = '{pipeline_id}' and variable_name = 'sheet_mapping'\"\n",
    "    df = spark.sql(sql)\n",
    "    new_id_row = df.collect()\n",
    "    sheet_mapping = new_id_row[0][\"variable_value\"] if new_id_row else None\n",
    "    try:\n",
    "        if sheet_mapping:\n",
    "            sheet_mapping = json.loads(sheet_mapping)\n",
    "            for sheet in sheet_mapping:\n",
    "                data_sheet = sheet.get('sheet_name')\n",
    "                p_p = sheet.get('destination_table').split(\".\")\n",
    "                table_name = p_p[0] + \".\" + p_p[1] + \".\" + sanitize_identifier(p_p[2]) \n",
    "                skip_rows = sheet.get('header_rows_to_skip')\n",
    "                footer_rows= sheet.get('footer_rows_to_skip')\n",
    "                data_sheet_name = data_sheet if data_sheet else detect_data_sheet(local_path,None)\n",
    "        \n",
    "                excl = pd.read_excel(local_path, sheet_name=data_sheet_name, skiprows = skip_rows)\n",
    "                excl = excl.dropna(axis=1, how='all')\n",
    "                if excl.empty:\n",
    "                    raise RuntimeError(\"Selected sheet is empty\")\n",
    "        \n",
    "                data_df = spark.createDataFrame(excl.astype(str))\n",
    "                data_df = normalize_columns(data_df)\n",
    "                data_df = (data_df\n",
    "                .withColumn(\"etl_id\", F.lit(new_id))\n",
    "                .withColumn(\"file_id\", F.lit(file_id))\n",
    "                )\n",
    "                (data_df.write.format(\"delta\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .saveAsTable(table_name)\n",
    "                )\n",
    "            sql = f\"SELECT variable_value FROM {environment}_011_bronze_core.db_admin.pipeline_parameter WHERE pipeline_id = '{pipeline_id}' and variable_name = 'archive_folder'\"\n",
    "            df = spark.sql(sql)\n",
    "            new_id_row = df.collect()\n",
    "            archive_folder = new_id_row[0][\"variable_value\"] if new_id_row else None \n",
    "            archive_folder = archive_folder.rstrip(\"/\") + \"/\" \n",
    "            print(f\"Loaded {file_name} -> {table_name}\")\n",
    "            dest_base = archive_folder\n",
    "            dest_path = f\"{dest_base.rstrip('/')}/{file_name}\"\n",
    "            dbutils.fs.mv(master_path, dest_path, True)\n",
    "        else:\n",
    "            print(\"missing_sheet_name_details\")\n",
    "            raise Exception(\"missing_sheet_name_details\")\n",
    "    except Exception as exc:\n",
    "        sql = f\"SELECT variable_value FROM {environment}_011_bronze_core.db_admin.pipeline_parameter WHERE pipeline_id = '{pipeline_id}' and variable_name = 'failure_folder'\"\n",
    "        df = spark.sql(sql)\n",
    "        new_id_row = df.collect()\n",
    "        failure_folder = new_id_row[0][\"variable_value\"] if new_id_row else None \n",
    "        failure_folder = failure_folder.rstrip(\"/\") + \"/\" \n",
    "        # errors.append((file_name, table_name, str(exc)))\n",
    "        print(f\"Failed {file_name} -> {table_name}: {exc}\")\n",
    "        dest_base = failure_folder\n",
    "        dest_path = f\"{dest_base.rstrip('/')}/{file_name}\"\n",
    "        dbutils.fs.mv(master_path, dest_path, True)\n",
    "        raise Exception(exc)\n",
    "except Exception as e:\n",
    "    # Handle outer try block exceptions\n",
    "    print(f\"Pipeline execution failed: {e}\")\n",
    "    raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cd8acb-4a18-42f3-9532-c973611eec12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/development_011_bronze_core/atlas/atlas/loading/December 2025 Action Poker Unit Count Data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# fileName = \"/Volumes/development_011_bronze_core/atlas/atlas/loading/December 2025 Action Poker Unit Count Data.xlsx\"\n",
    "# print(fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2fceac-d989-41c9-b7e7-cf2a930458a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dbutils.fs.mv('s3://cluster-private-bucket-481980074735-us-east-1/intake/atlas/failure/excel/Panam√° Seguimiento IGT-Sierra 27 Octubre 2025.xlsx/', 's3://cluster-private-bucket-481980074735-us-east-1/intake/atlas/manual/', True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Job_IGTEXCEL_BRONZE",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}